% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/logit_mixDirLapl_mcmc.R
\name{logit_mixDirLapl_mcmc}
\alias{logit_mixDirLapl_mcmc}
\title{Logit Model with Mixture of Dirichlet-Laplace Priors: Markov Chain
Monte Carlo Algorithm}
\usage{
logit_mixDirLapl_mcmc = function(nsim, X, y, alpha, start = NULL,
                                     burn = 0, thin = 1, verbose = +Inf)
}
\arguments{
\item{nsim}{The number of simulations.}

\item{X}{The design matrix.}

\item{y}{The response vector.}

\item{start}{The starting point.}

\item{burn}{The number of draws to be discarded as burn-in.}

\item{thin}{The thinning parameter.}

\item{verbose}{The period for printing status of the chain.}
}
\value{
Posterior sample of the parameters.
}
\description{
Gibbs sampler algorithm for the logit model with no intercept. A
mixture of Dirichlet-Laplace prior is used for the vector of the
coefficients.
}
\details{
Logistic regression model, say
\deqn{
 y_i \overset{ind}{\sim} Bernoulli (X^\prime_i \beta)
}
where \eqn{X^\prime_i} is the \eqn{i}-th row of the design matrix \eqn{X}.
The prior on the coefficient vector \eqn{\beta} is a mixture of the
Dirichlet-Laplace priors. Conditionally on the hyperparameter \eqn{\alpha},
the prior is Dirichlet-Laplace that is:
\deqn{
 \beta_i \overset{ind}{\sim} N(0, \sigma^2 \delta^2_i \psi_i) \, , \,
  \delta_i \overset{i.i.d.}{\sim} \mathrm{gamma}(\alpha, 1 / 2)
  \perp\!\!\!\perp \psi_i \overset{i.i.d.}{\sim} \exp(1 / 2)
 }
or, equivalently
\deqn{
  \beta_i \overset{ind}{\sim} N(0, \sigma^2 \tau^2 \phi^2_i \psi_i) \\
  \tau \sim \mathrm{gamma}(n \alpha, 1 / 2) \\
  \phi \sim \mathrm{Dirichlet}_n(\alpha, \alpha, \dots, \alpha) \\
  \psi_i \overset{i.i.d.}{\sim} \exp(1 / 2)
 }
and the prior of \eqn{\alpha} is uniform on the discrete support
\deqn{
 \frac{1}{p} + (\frac{1}{2} - \frac{1}{p}) \frac{i}{199} \, , \, i = 0, 1,
 \dots, 199
}
say, the support of \eqn{\alpha} is \code{seq(from = 1 / p, to = 0.5,
length.out = 200)}.
For the Gibbs sampler, it is used the first representation.

\code{start}: the starting point is generated in the following way:
\itemize{
\item if \code{start == NULL} then set the starting point of \code{beta}
from elastic net.
\item \code{else} set the starting point for \code{beta} to \code{start}.
}

Only one value every \code{thin} values is kept in the chain, so the true
number of complete scans will be \code{nsim * thin + burn}. By default
\code{thin = 1}, that is no thinning.

The current time and the current number of iteration are printed one every
\code{verbose} iterations. Furthermore:
\itemize{
\item if \code{verbose == +-Inf} then there is no printing,
\item if \code{verbose != +-Inf} then at least start and end of simulation
are reported.
}
}
