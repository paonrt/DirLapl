% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lm_DirLapl_mcmc.R
\name{lm_DirLapl_mcmc}
\alias{lm_DirLapl_mcmc}
\title{Linear Model with Dirichlet-Laplace Prior: Markov Chain Monte Carlo
Algorithm}
\usage{
lm_DirLapl_mcmc = function(nsim, X, y, alpha, shape_prior, rate_prior,
                                  start = NULL, burn = 0, thin = 1,
                                  verbose = +Inf)
}
\arguments{
\item{nsim}{The number of simulations.}

\item{X}{The design matrix.}

\item{y}{The response vector.}

\item{alpha}{The hyper-parameter controlling the amount of shrinkage.}

\item{shape_prior}{Shape parameter for the prior of variance.}

\item{rate_prior}{Rate parameter for the prior of variance.}

\item{start}{The starting point.}

\item{burn}{The number of draws to be discarded as burn-in.}

\item{thin}{The thinning parameter.}

\item{verbose}{The period for printing status of the chain.}
}
\value{
Posterior sample of the parameters.
}
\description{
Gibbs sampler algorithm for the linear model with no intercept
and unknown variance. A Dirichlet-Laplace prior is used for the vector of
the coefficients and an inverse gamma prior for the variance.
}
\details{
Linear regression model with no intercept and unknown variance, say
\deqn{
 y \sim N_n (X \beta, \sigma^2 I_n)
}
where \eqn{I_n} is the \eqn{n}-dimensional identity matrix. The prior on the
coefficient vector \eqn{\beta} is the Dirichlet-Laplace prior:
\deqn{
 \beta_i \overset{ind}{\sim} N(0, \sigma^2 \delta^2_i \psi_i) \, , \,
  \delta_i \overset{i.i.d.}{\sim} \mathrm{gamma}(\alpha, 1 / 2)
  \perp\!\!\!\perp \psi_i \overset{i.i.d.}{\sim} \exp(1 / 2)
 }
or, equivalently
\deqn{
  \beta_i \overset{ind}{\sim} N(0, \sigma^2 \tau^2 \phi^2_i \psi_i) \\
  \tau \sim \mathrm{gamma}(n \alpha, 1 / 2) \\
  \phi \sim \mathrm{Dirichlet}_n(\alpha, \alpha, \dots, \alpha) \\
  \psi_i \overset{i.i.d.}{\sim} \exp(1 / 2)
 }
and the prior of \eqn{\sigma^2} is an inverse gamma distribution with shape
\code{shape_prior} and rate \code{rate_prior}.
For the Gibbs sampler, it is used the first representation.

\code{start}: the starting point is generated in the following way:
\itemize{
\item if \code{start == NULL} then set the starting point of \code{beta} and
\code{sigmaSq} from elastic net.
\item \code{else} set the starting point for \code{beta}, \code{sigmaSq}
equal to \code{start$beta}, \code{start$sigmaSq}.
}

Only one value every \code{thin} values is kept in the chain, so the true
number of complete scans will be \code{nsim * thin + burn}. By default
\code{thin = 1}, that is no thinning.

The current time and the current number of iteration are printed one every
\code{verbose} iterations. Furthermore:
\itemize{
\item if \code{verbose == +-Inf} then there is no printing,
\item if \code{verbose != +-Inf} then at least start and end of simulation
are reported.
}
}
